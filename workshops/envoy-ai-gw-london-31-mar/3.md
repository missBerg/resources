# Add Ollama as a Target

## Installing Ollama

### Download
To install Ollamavisit ollama\.com\/download and download the installer for your operating system \(Windows\, macOS\, or Linux\)\, then run the installer and follow the on\-screen instructions\.

### Start Ollama:
```shell
ollama serve
```


### Pull the mistral:latest model:
```shell
ollama pull mistral:latest
```


### Test Ollama locally:
```shell
curl http://localhost:11434/api/chat -d '{
 "model": "mistral:latest",
 "messages": [
   { "role": "user", "content": "why is the sky blue?" }
 ]
}'
```

## Add to Gateway
### Create Ollama backend configuration\.

Add the following to your basic\.yaml file\:
```yaml
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
 name: ollama-backend
 namespace: default
spec:
 schema:
   name: OpenAI
 backendRef:
   name: ollama-service
   kind: Service
   port: 11434
---
apiVersion: v1
kind: Service
metadata:
 name: ollama-service
 namespace: default
spec:
 type: ExternalName
 # For Kind use kind-control-plane
 # For Docker desktop use host.docker.internal
 externalName: host.docker.internal
 ports:
   - port: 11434


```
Apply configuration\:
```warp-runnable-command
kubectl apply -f basic.yaml
```

## Add Ollama Mistral as Backend

Open basic\.yaml and add the ollama backend to the AI Gateway route\:
```yaml
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIGatewayRoute
metadata:
 name: envoy-ai-gateway-basic
 namespace: default
spec:
 schema:
   name: OpenAI
 targetRefs:
   - name: envoy-ai-gateway-basic
     kind: Gateway
     group: gateway.networking.k8s.io
 rules:
   - matches:
       - headers:
           - type: Exact
             name: x-ai-eg-model
             value: gpt-4o-mini
     backendRefs:
       - name: envoy-ai-gateway-basic-openai
   - matches:
       - headers:
           - type: Exact
             name: x-ai-eg-model
             value: us.meta.llama3-2-1b-instruct-v1:0
     backendRefs:
       - name: envoy-ai-gateway-basic-aws
   - matches:
       - headers:
           - type: Exact
             name: x-ai-eg-model
             value: some-cool-self-hosted-model
     backendRefs:
       - name: envoy-ai-gateway-basic-testupstream
   - matches:
       - headers:
           - type: Exact
             name: x-ai-eg-model
             value: mistral:latest
     backendRefs:
       - name: ollama-backend
---
```


Apply configuration\:
```warp-runnable-command
kubectl apply -f basic.yaml
```

## Make a Test call
```warp-runnable-command
curl http://localhost/v1/chat/completions -d '{
        "model": "mistral:latest",
        "messages": [
            {
                "role": "user",
                "content": "What is the capital of the moon?"
            },
            {
                "role": "assistant",
                "content": "The capital of the moon is the moon."
            },
            {
                "role": "user",
                "content": "What is the capital of the moon?"
            }
        ]
       }'
```
